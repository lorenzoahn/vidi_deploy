<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Video Agent for BLV users</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/14.8.49/Tone.js"></script>

  <style>
  body {
    font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display", "Helvetica Neue", Arial, sans-serif;
    text-align: start;
    margin: auto;
    display: flex;
    flex-direction: column;
    width: 40%;
    justify-content: center;
    align-items: center;
    min-height: 100vh;
    background-color: #f9f9f9;
    padding-bottom: 80px;
  }

  .container {
    width: 60%;
    max-width: 800px;
    background: white;
    padding: 20px;
    border-radius: 10px;
    box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
    text-align: start;
  }

  video, canvas {
    width: 100%;
    margin: 20px 0;
    border: 1px solid #ddd;
    border-radius: 5px;
  }

  #description, #answer {
    margin-top: 20px;
    font-weight: bold;
  }
  </style>

</head>
<body>

  <!-- Video Title and Description -->
  <h2>Video Title: ANY iPhone How To Add Filter on Camera</h2>
  <p>
    iPhone camera filter | How to add filter on iPhone camera! 
    Learn how to add a filter to your iPhone camera in this tutorial.
  </p>

  <!-- Video Player -->
  <video id="videoPlayer" width="640" height="360" controls crossorigin="anonymous">
    <source src="/static/video.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

  <!-- Canvas for Keyframe -->
  <canvas id="canvas"></canvas>

  <!-- Description Output -->
  <div id="description">Description will appear here...</div>
  <div id="answer">Answer will appear here...</div>

  <script>
    const video = document.getElementById("videoPlayer");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const descriptionDiv = document.getElementById("description");
    const answerDiv = document.getElementById("answer");
    let currentBase64Image = null;
    let speechUtterance = null;
    let recognition = null;

    const beepTimes = [18, 45, 57];
    const missedBeepMessages = {
      18: "The filter button is here.",
      45: "Where do I go to see saved photos? Or a little before, the person doesn’t verbally describe that they’re tapping somewhere else on the screen at all.",
      57: "How do I change the intensity? Where is the slider?",
    };
    let playedBeeps = new Set();
    let lastBeepTime = null;

    // Tone.js synth for beep
    const synth = new Tone.Synth({ oscillator: { type: "sine" } }).toDestination();

    function playBeep() {
      Tone.start();
      synth.triggerAttackRelease("C6", "8n");
    }

    function notifyMissedBeep(beepTime) {
      if (missedBeepMessages[beepTime]) {
        window.speechSynthesis.cancel();
        const message = missedBeepMessages[beepTime];
        const utterance = new SpeechSynthesisUtterance(message);
        window.speechSynthesis.speak(utterance);
      }
    }

    // Play beep at specific times
    video.addEventListener("timeupdate", () => {
      const currentTime = Math.floor(video.currentTime);
      if (beepTimes.includes(currentTime) && !playedBeeps.has(currentTime)) {
        playBeep();
        playedBeeps.add(currentTime);
        lastBeepTime = currentTime;
      }
    });

    // Detect when the user pauses within 2-3 seconds of a beep
    video.addEventListener("pause", () => {
      if (lastBeepTime !== null) {
        const currentTime = Math.floor(video.currentTime);
        if (currentTime >= lastBeepTime + 2 && currentTime <= lastBeepTime + 3) {
          notifyMissedBeep(lastBeepTime);
        }
      }
    });

    // Extract frame when video is paused
    video.addEventListener("pause", async () => {
      if (video.paused) {
        try {
          canvas.width = 640;
          canvas.height = 360;
          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
          currentBase64Image = canvas.toDataURL("image/jpeg").split(",")[1];

          const description = await getDescriptionFromBackend(currentBase64Image);
          descriptionDiv.textContent = description || "Failed to generate description.";
        } catch (error) {
          console.error("Error capturing frame:", error);
          descriptionDiv.textContent = "Error generating description.";
        }
      }
    });

    async function getDescriptionFromBackend(base64Image) {
      try {
        const response = await fetch("/process-image", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ image: base64Image }),
        });

        if (!response.ok) throw new Error("Error fetching description from backend.");

        const data = await response.json();
        return data.description || "No description available.";
      } catch (error) {
        console.error("Error:", error);
        return "Error generating description.";
      }
    }

    function startVoiceRecognition() {
      if (!currentBase64Image) {
        alert("Pause the video first to capture a keyframe.");
        return;
      }

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        alert("Speech recognition is not supported in this browser.");
        return;
      }

      recognition = new SpeechRecognition();
      recognition.lang = "en-US";
      recognition.continuous = false;
      recognition.interimResults = false;

      recognition.onstart = () => {
        console.log("Voice recognition started. Speak now...");
      };

      recognition.onresult = async (event) => {
        const question = event.results[0][0].transcript;
        console.log("Recognized Question:", question);
        answerDiv.textContent = "Processing your question...";

        const answer = await askQuestion(currentBase64Image, question);
        answerDiv.textContent = answer;
        readAnswerAloud(answer);
      };

      recognition.onerror = (event) => {
        console.error("Speech recognition error:", event.error);
        answerDiv.textContent = "Error recognizing speech. Try again.";
      };

      recognition.start();
    }

    async function askQuestion(base64Image, question) {
      try {
        const response = await fetch("/process-question", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ image: base64Image, question }),
        });

        if (!response.ok) throw new Error("Error fetching answer from backend.");

        const data = await response.json();
        return data.answer || "No answer available.";
      } catch (error) {
        console.error("Error:", error);
        return "Error generating answer.";
      }
    }

    function readAnswerAloud(answer) {
      window.speechSynthesis.cancel();
      speechUtterance = new SpeechSynthesisUtterance(answer);
      window.speechSynthesis.speak(speechUtterance);
    }

    // Keyboard controls
    document.addEventListener("keydown", (event) => {
      if (event.key.toLowerCase() === "d") {
        readDescriptionAloud();
      } else if (event.key.toLowerCase() === "a") {
        startVoiceRecognition();
      } else if (event.key === "Escape") {
        window.speechSynthesis.cancel();
      } else if (event.code === "Space") {
        event.preventDefault();
        video.paused ? video.play() : video.pause();
      }
    });

    // Initial screen reader message
    const initialMessage = new SpeechSynthesisUtterance(
      "Welcome! For the best experience, consider turning off your screen reader. This assistant will provide spoken descriptions automatically."
    );
    window.speechSynthesis.speak(initialMessage);

  </script>
</body>
</html>
