<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Video Chat: Video Assistant Agent for Blind and Low Vision Users</title>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/14.8.49/Tone.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin-top: 50px;
    }
    video, canvas {
      margin: 20px auto;
      display: block;
      border: 1px solid #ddd;
    }
    #description, #answer {
      margin-top: 20px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>Video Chat: Video Assistant Agent for Blind and Low Vision Users</h1>

  <p>
    1. Press ESC to stop reading the description.<br>
    2. Press D after description has loaded to read it aloud.<br>
    3. Press Q to ask a question using voice input.<br>
    4. Press SPACE to play/pause the video.
  </p>

  <!-- Video Player -->
  <video id="videoPlayer" width="640" height="360" controls crossorigin="anonymous">
    <source src="/static/video.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

  <!-- Canvas for Keyframe -->
  <canvas id="canvas"></canvas>

  <!-- Description Output -->
  <div id="description">Description will appear here...</div>
  <div id="answer">Answer will appear here...</div>

  <script>
    const video = document.getElementById("videoPlayer");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const descriptionDiv = document.getElementById("description");
    const answerDiv = document.getElementById("answer");
    let currentBase64Image = null;
    let speechUtterance = null;
    let recognition = null;

    const beepTimes = [18, 45, 57]; // Beep timestamps in seconds
    const missedBeepMessages = {
      18: "The filter button is here.",
      45: "Where do I go to see saved photos? Or a little before, the person doesn’t verbally describe that they’re tapping somewhere else on the screen at all.",
      57: "how do i change the intensity, where is the slider",
    };
    let playedBeeps = new Set();
    let lastBeepTime = null; // Track last beep time

    // Tone.js synth for beep
    const synth = new Tone.Synth({
      oscillator: { type: "sine" }
    }).toDestination();

    function playBeep() {
      Tone.start();
      synth.triggerAttackRelease("C6", "8n"); // Short beep sound
    }

    function notifyMissedBeep(beepTime) {
      if (missedBeepMessages[beepTime]) {
        window.speechSynthesis.cancel(); // Stop any ongoing speech
        const message = missedBeepMessages[beepTime];
        const utterance = new SpeechSynthesisUtterance(message);
        window.speechSynthesis.speak(utterance);
      }
    }

    // Listen for video time updates to play beep
    video.addEventListener("timeupdate", () => {
      const currentTime = Math.floor(video.currentTime);

      if (beepTimes.includes(currentTime) && !playedBeeps.has(currentTime)) {
        playBeep();
        playedBeeps.add(currentTime);
        lastBeepTime = currentTime; // Store last beep timestamp
      }
    });

    // Detect when the user pauses the video
    video.addEventListener("pause", () => {
      if (lastBeepTime !== null) {
        const currentTime = Math.floor(video.currentTime);
        if (currentTime >= lastBeepTime + 2 && currentTime <= lastBeepTime + 3) {
          notifyMissedBeep(lastBeepTime);
        }
      }
    });

    // Event Listener: Extract frame when video is paused
    video.addEventListener("pause", async () => {
      if (video.paused) {
        try {
          canvas.width = 640;
          canvas.height = 360;
          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
          currentBase64Image = canvas.toDataURL("image/jpeg").split(",")[1];

          const description = await getDescriptionFromBackend(currentBase64Image);
          descriptionDiv.textContent = description || "Failed to generate description.";
        } catch (error) {
          console.error("Error capturing frame:", error);
          descriptionDiv.textContent = "Error generating description.";
        }
      }
    });

    async function getDescriptionFromBackend(base64Image) {
      try {
        const response = await fetch("/process-image", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ image: base64Image }),
        });

        if (!response.ok) throw new Error("Error fetching description from backend.");

        const data = await response.json();
        return data.description || "No description available.";
      } catch (error) {
        console.error("Error:", error);
        return "Error generating description.";
      }
    }

    function readDescriptionAloud() {
      const description = descriptionDiv.textContent;
      if (description && description !== "Description will appear here...") {
        speechUtterance = new SpeechSynthesisUtterance(description);
        window.speechSynthesis.speak(speechUtterance);
      } else {
        console.error("No description available to read.");
      }
    }

    function stopDescriptionReading() {
      window.speechSynthesis.cancel();
    }

    function startVoiceRecognition() {
      if (!currentBase64Image) {
        alert("Pause the video first to capture a keyframe.");
        return;
      }

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        alert("Speech recognition is not supported in this browser.");
        return;
      }

      recognition = new SpeechRecognition();
      recognition.lang = "en-US";
      recognition.continuous = false;
      recognition.interimResults = false;

      recognition.onstart = () => {
        console.log("Voice recognition started. Speak now...");
      };

      recognition.onresult = async (event) => {
        const question = event.results[0][0].transcript;
        console.log("Recognized Question:", question);
        answerDiv.textContent = "Processing your question...";

        const answer = await askQuestion(currentBase64Image, question);
        answerDiv.textContent = answer;
        readAnswerAloud(answer);
      };

      recognition.onerror = (event) => {
        console.error("Speech recognition error:", event.error);
        answerDiv.textContent = "Error recognizing speech. Try again.";
      };

      recognition.start();
    }

    async function askQuestion(base64Image, question) {
      try {
        const response = await fetch("/process-question", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ image: base64Image, question }),
        });

        if (!response.ok) throw new Error("Error fetching answer from backend.");

        const data = await response.json();
        return data.answer || "No answer available.";
      } catch (error) {
        console.error("Error:", error);
        return "Error generating answer.";
      }
    }

    function readAnswerAloud(answer) {
      window.speechSynthesis.cancel();
      speechUtterance = new SpeechSynthesisUtterance(answer);
      window.speechSynthesis.speak(speechUtterance);
    }

  </script>
</body>
</html>
